
# AntiDeepfake

AntiDeepfake — это система для детекции DeepFake видео, использующая передовые нейронные сети и модели машинного обучения. Этот проект включает в себя инструменты для обнаружения фальшивых видеозаписей с помощью нескольких методов анализа, таких как анализ признаков лиц, оптический поток, частотный анализ и другие.

## Особенности

- Использует модели `EfficientNet`, `Xception`, и `CLIP` для извлечения признаков.
- Включает в себя несколько различных подходов к детекции: анализ лиц, потока, частоты, моргания глаз и синхронизации губ.
- Модель обучается с использованием большого набора данных с метками "реальное" и "фальшивое".
- Поддержка работы с видеофайлами и их анализ в реальном времени.

## Установка

### Требования

- Docker

### Установка зависимостей

1. Клонируйте репозиторий:

   ```bash
   git clone https://github.com/IvanKonovalenko/AntiDeepfake.git
   cd AntiDeepfake
   ```

2. Убедитесь, что у вас установлен Docker и Docker Compose. Вы можете загрузить и установить их с официальных сайтов:
   - [Docker](https://www.docker.com/get-started)
   - [Docker Compose](https://docs.docker.com/compose/install/)

### Настройка API ключа для Telegram бота

Для использования Telegram бота необходимо указать API ключ в файле конфигурации.

1. Перейдите в файл `src/TgBot/TgBot/appsettings.json`.
2. Укажите ваш API ключ в поле `"BotToken"`:

```json
{
  "BotToken": "YOUR_TELEGRAM_API_KEY"
}
```

### Запуск проекта с помощью Docker и Docker Compose

1. Для запуска проекта с помощью Docker и Docker Compose выполните следующие команды:

   ```bash
   docker-compose build
   docker-compose up
   ```

   Эти команды создадут и запустят контейнеры для вашего проекта. После этого приложение будет доступно для использования.

## Архитектура

Проект использует несколько моделей для извлечения признаков:

1. **EfficientNet** для извлечения пространственных признаков.
2. **Xception** для дополнительной обработки изображений.
3. **CLIP** от OpenAI для обработки изображений с помощью трансформеров.
4. **Temporal Transformer** для обработки временных данных и анализа динамики видео.

Все модели работают совместно для создания единого ансамблевого подхода, что позволяет повысить точность детекции.

## Вклад

Если вы хотите внести свой вклад в проект, пожалуйста, следуйте этим шагам:

1. Форкните репозиторий.
2. Создайте свою ветку (`git checkout -b feature/your-feature`).
3. Коммитьте свои изменения (`git commit -am 'Add new feature'`).
4. Отправьте пулл-реквест.


